{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=os.listdir('Training/Tasks')\n",
    "tasks=tasks[1:-1]\n",
    "\n",
    "\n",
    "running=True\n",
    "i=0\n",
    "while ((running==True) and (i<=len(tasks))):\n",
    "    task=tasks[0]\n",
    "    running=os.path.exists(\"Training/Tasks/Running/\"+task)\n",
    "    i=i+1\n",
    "with open('Training/Tasks/Running/'+task, 'wb') as handle:\n",
    "    pickle.dump(0, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if i>len(tasks):\n",
    "    print ('all done . . . ')\n",
    "else: \n",
    "    print ('Continue . . .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Training/Tasks/'+task , 'rb') as file:\n",
    "    model_version=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chars2vec\n",
    "#with tf.device('/gpu:0'):\n",
    "import random\n",
    "chars2vec_model = chars2vec.load_model('eng_200')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import json\n",
    "import string\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "#with tf.device('/gpu:0'):\n",
    "db_name = '../../../data/Bib2Auth/data1.db'\n",
    "global conn\n",
    "conn = sqlite3.connect(db_name,check_same_thread=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56980288, 0.45584231, 0.68376346],\n",
       "       [0.26726124, 0.53452248, 0.80178373]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[5,4,6],[1,2,3]]\n",
    "tf.keras.utils.normalize(a, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3. , 3. , 4.5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((a[0],a[1]), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, test and validation daataset\n",
    "#with tf.device('/gpu:0'):\n",
    "with open('Training/Miscs/new_authors_ids_'+str(model_version)+'.pickle','rb') as file:\n",
    "    new_authors_ids=pickle.load(file)\n",
    "with open('Training/Miscs/old_authors_ids_'+str(model_version)+'.pickle','rb') as file:\n",
    "    old_authors_ids=pickle.load(file)\n",
    "with open('Training/Data/x_train_new_'+str(model_version)+'.pickle','rb') as file:\n",
    "    x_train=pickle.load(file)\n",
    "with open('Training/Data/y_train_new_'+str(model_version)+'.pickle','rb') as file:\n",
    "    y_train=pickle.load(file)\n",
    "with open('Training/Data/x_validate_new_'+str(model_version)+'.pickle','rb') as file:\n",
    "    x_validate=pickle.load(file)\n",
    "with open('../../../data/Bib2Auth/auth2index.pickle','rb') as file:\n",
    "    auth2index=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_authors_ids=list(old_authors_ids)\n",
    "new_authors_ids=list(new_authors_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dict_factory(cursor, row):\n",
    "    d = {}\n",
    "    for idx, col in enumerate(cursor.description):\n",
    "        d[col[0]] = row[idx]\n",
    "    return d\n",
    "\n",
    "try:\n",
    "    with open('Training/Miscs/records_train_'+str(model_version)+'.pickle', 'wb') as handle:\n",
    "        records_train=pickle.load(handle)\n",
    "except:\n",
    "    conn.row_factory = dict_factory\n",
    "    query = \"SELECT * FROM reference WHERE combination_id in \"+str(tuple(x_train))\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    records_train = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    with open('Training/Miscs/records_train_'+str(model_version)+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(records_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Training/Miscs/records_validate_'+str(model_version)+'.pickle', 'wb') as handle:\n",
    "        records_validate=pickle.load(handle)\n",
    "except:\n",
    "    conn.row_factory = dict_factory\n",
    "    query = \"SELECT * FROM reference WHERE combination_id in \"+str(tuple(x_validate))\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    records_validate = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    if (conn):\n",
    "        conn.close()\n",
    "    with open('Training/Miscs/records_validate_'+str(model_version)+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(records_validate, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_variants(auth_fname,auth_lname,coauth_fname,coauth_lname):\n",
    "\n",
    "    # Remove all the punctuations and convert to lowercase\n",
    "    auth_fname = auth_fname.translate(str.maketrans('', '', string.punctuation)).casefold()\n",
    "    auth_lname = auth_lname.translate(str.maketrans('', '', string.punctuation)).casefold()\n",
    "    coauth_fname = coauth_fname.translate(str.maketrans('', '', string.punctuation)).casefold()\n",
    "    coauth_lname = coauth_lname.translate(str.maketrans('', '', string.punctuation)).casefold()\n",
    "\n",
    "    auth_name = auth_fname + \" \" + auth_lname\n",
    "    coauth_name = coauth_fname + \" \" + coauth_lname\n",
    "\n",
    "    combo_list = []\n",
    "    auth_name_variants = []\n",
    "    coauth_name_variants = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    #auth_name_variants.append(auth_name)\n",
    "    #coauth_name_variants.append(coauth_name)\n",
    "    \n",
    "    # Check if author last name is missing and generate varinats accordingly\n",
    "    if(len(auth_lname) == 0):\n",
    "        auth_name_variants.append(auth_name)\n",
    "        auth_name_variants.append(auth_name[0])\n",
    "    else:\n",
    "        auth_name_variants.append(auth_name)\n",
    "        auth_name_variants.append(auth_fname[0]+ ' ' + auth_lname)\n",
    "        #auth_name_variants.append(auth_fname + ' ' + auth_lname[0])\n",
    "\n",
    "    # Check if co-author last name is missing and generate varinats accordingly\n",
    "    if(len(coauth_lname) == 0):\n",
    "        coauth_name_variants.append(coauth_name)\n",
    "        coauth_name_variants.append(coauth_name[0])\n",
    "    else:\n",
    "        coauth_name_variants.append(coauth_name)\n",
    "        coauth_name_variants.append(coauth_fname[0]+ ' ' + coauth_lname)\n",
    "        #coauth_name_variants.append(coauth_fname + ' ' + coauth_lname[0])\n",
    " \n",
    "    # Check if author and co-author are same and generate combos accordingly\n",
    "    if(auth_name == coauth_name):\n",
    "        combo_list = [(name,name) for name in auth_name_variants]\n",
    "    else:\n",
    "        combo_list = list(itertools.product(auth_name_variants,coauth_name_variants))\n",
    "    return combo_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(combo):\n",
    "    author_name = combo[0]\n",
    "    \n",
    "    try:\n",
    "        with open('Training/tdata/auth_data/'+author_name+'.pickle','rb') as file:\n",
    "            auth_emb=pickle.load(file)\n",
    "    except:\n",
    "        auth_emb = chars2vec_model.vectorize_words([author_name])[0]\n",
    "        with open('Training/tdata/auth_data/'+author_name+'.pickle','wb') as file:\n",
    "            pickle.dump(auth_emb,file)\n",
    "\n",
    "    return auth_emb\n",
    "    \n",
    "def get_emb2(combo,record):\n",
    "    co_author_name = combo[1]\n",
    "    title = record[7]\n",
    "\n",
    "    # take journal full form else abbr journal if full form not avaialable\n",
    "    journal = record[10] if len(record[10])>0 else record[9]\n",
    "\n",
    "    # Tranform each attribute to corresponding embeddings\n",
    "    #s=time.time()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('Training/tdata/auth_data/'+co_author_name+'.pickle','rb') as file:\n",
    "            coauth_emb=pickle.load(file)\n",
    "    except:\n",
    "        coauth_emb = chars2vec_model.vectorize_words([co_author_name])[0]\n",
    "        with open('Training/tdata/auth_data/'+co_author_name+'.pickle','wb') as file:\n",
    "            pickle.dump(coauth_emb,file)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('Training/tdata/ref_data/title'+str(record[1])+'.pickle','rb') as file:\n",
    "            title_emb=pickle.load(file)\n",
    "    except:\n",
    "        title_emb = bert_model.encode(title)\n",
    "        with open('Training/tdata/ref_data/title'+str(record[1])+'.pickle','wb') as file:\n",
    "            pickle.dump(title_emb,file)\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        with open('Training/tdata/ref_data/source'+str(record[1])+'.pickle','rb') as file:\n",
    "                journal_emb=pickle.load(file)\n",
    "    except:\n",
    "        journal_emb = bert_model.encode(journal)\n",
    "        with open('Training/tdata/ref_data/source'+str(record[1])+'.pickle','wb') as file:\n",
    "            pickle.dump(journal_emb,file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    content_emb=np.sum((title_emb,journal_emb), axis=0)               #NEW LINE\n",
    "    #return np.concatenate([coauth_emb,title_emb,journal_emb])\n",
    "    #return np.concatenate([auth_emb,coauth_emb,content_emb])\n",
    "    return np.concatenate([coauth_emb,content_emb])\n",
    "    #return auth_emb\n",
    "\n",
    "def generate_embeddings2(record):   \n",
    "    embeddings_list = []\n",
    "    embeddings_list2 = []\n",
    "    labels_list = []\n",
    "    \n",
    "    #there were a mistake in the obtention of fname and lname\n",
    "    a1=record[2].split()\n",
    "    a1_fname=' '.join(a1[:-1])\n",
    "    a1_lname=a1[-1]\n",
    "    if not bool(a1_fname):\n",
    "        a1_fname=a1_lname\n",
    "    a2=record[5]+' '+record[6]\n",
    "    a2=a2.split()\n",
    "    a2_fname=' '.join(a2[:-1])\n",
    "    a2_lname=a2[-1]\n",
    "    if not bool(a2_fname):\n",
    "        a2_fname=a2_lname\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the combinations of name variants\n",
    "    combo_list = generate_name_variants(a1_fname,a1_lname,a2_fname,a2_lname)\n",
    "\n",
    "    # Get the target author index\n",
    "    target_author_name = record[2]\n",
    "    tmp=auth2index[target_author_name]\n",
    "    target_author_ind=new_authors_ids[old_authors_ids.index(tmp)]\n",
    "    #target_author_ind = auth2index[target_author_name]\n",
    "    \n",
    "    tmp=[(embeddings_list.append(get_emb(combo)),embeddings_list2.append(get_emb2(combo,record))) for combo in combo_list]\n",
    "    labels_list=[target_author_ind]*len(embeddings_list)\n",
    "\n",
    "    embeddings_list = np.array(embeddings_list)\n",
    "    embeddings_list2 = np.array(embeddings_list2)\n",
    "    labels_list = np.array(labels_list)\n",
    "    embeddings_list = tf.keras.utils.normalize(embeddings_list, axis = 1)\n",
    "    embeddings_list2 = tf.keras.utils.normalize(embeddings_list2, axis = 1)\n",
    "\n",
    "    return embeddings_list,embeddings_list2,labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_records_at(feature_id,t):\n",
    "    if t==1:\n",
    "        record=tuple(records_train[feature_id].values())\n",
    "    elif t==2:\n",
    "        record=tuple(records_validate[feature_id].values())\n",
    "    return generate_embeddings2(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start=time.time()\n",
    "\n",
    "feature_id=25\n",
    "record=tuple(records_train[feature_id].values())\n",
    "#tmp=[i for i in range (len(ref_train)) if ref_train[i]==record[2]]\n",
    "#feature_id2=random.choice(tmp)\n",
    "#record2=tuple(records_train[feature_id2].values())\n",
    "#feature_id3=random.choice(tmp)\n",
    "#record3=tuple(records_train[feature_id3].values())\n",
    "\n",
    "tmp=np.where(ref_train==auth2index[record[2]])\n",
    "feature_id2=random.choice(tmp[0])\n",
    "record2=tuple(records_train[feature_id2].values())\n",
    "feature_id3=random.choice(tmp[0])\n",
    "record3=tuple(records_train[feature_id3].values())\n",
    "    \n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"Training/tdata/\"+str(model_version)+'/train', exist_ok=True)\n",
    "os.makedirs(\"Training/tdata/\"+str(model_version)+'/validate', exist_ok=True)\n",
    "\n",
    "def train_data_generator():\n",
    "    xx=list(range(len(x_train)))\n",
    "    random.shuffle(xx)\n",
    "    for feature_id in xx:\n",
    "        \n",
    "        try:\n",
    "            with open('Training/tdata/'+str(model_version)+'/train/X_'+str(feature_id)+'.pickle','rb') as file:\n",
    "                X=pickle.load(file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/train/X2_'+str(feature_id)+'.pickle','rb') as file:\n",
    "                X2=pickle.load(file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/train/Y_'+str(feature_id)+'.pickle','rb') as file:\n",
    "                Y=pickle.load(file)\n",
    "        except: \n",
    "            X,X2,Y = read_records_at(feature_id,t=1)\n",
    "            with open('Training/tdata/'+str(model_version)+'/train/X_'+str(feature_id)+'.pickle','wb') as file:\n",
    "                pickle.dump(X,file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/train/X2_'+str(feature_id)+'.pickle','wb') as file:\n",
    "                pickle.dump(X2,file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/train/Y_'+str(feature_id)+'.pickle','wb') as file:\n",
    "                pickle.dump(Y,file)\n",
    "    \n",
    "        for x,x2,y in zip(X,X2,Y):\n",
    "            yield {\"input_1\": x, \"input_2\": x2},y\n",
    "            #yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_data_generator():\n",
    "\n",
    "    for feature_id in range(len(x_validate)):\n",
    "        \n",
    "        try:\n",
    "            with open('Training/tdata/'+str(model_version)+'/validate/X_'+str(feature_id)+'.pickle','rb') as file:\n",
    "                X=pickle.load(file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/validate/X2_'+str(feature_id)+'.pickle','rb') as file:\n",
    "                X2=pickle.load(file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/validate/Y_'+str(feature_id)+'.pickle','rb') as file:\n",
    "                Y=pickle.load(file)\n",
    "        except: \n",
    "            X,X2,Y = read_records_at(feature_id,t=2)\n",
    "            with open('Training/tdata/'+str(model_version)+'/validate/X_'+str(feature_id)+'.pickle','wb') as file:\n",
    "                pickle.dump(X,file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/validate/X2_'+str(feature_id)+'.pickle','wb') as file:\n",
    "                pickle.dump(X2,file)\n",
    "            with open('Training/tdata/'+str(model_version)+'/validate/Y_'+str(feature_id)+'.pickle','wb') as file:\n",
    "                pickle.dump(Y,file)\n",
    "\n",
    "        for x,x2,y in zip(X,X2,Y):\n",
    "            yield {\"input_1\": x, \"input_2\": x2},y\n",
    "            #yield x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPOCHS = 500\n",
    "_PATIENCE = 50\n",
    "_BATCH_SIZE = 32\n",
    "\n",
    "_STEPS_PER_EPOCH = len(x_train)*4 // _BATCH_SIZE  \n",
    "_VALIDATION_STEPS = len(x_validate)*4 // _BATCH_SIZE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(train_data_generator, ({\"input_1\": tf.float32, \"input_2\": tf.float32}, tf.int64))\n",
    "\n",
    "train_dataset = train_dataset.batch(_BATCH_SIZE)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "validate_dataset = tf.data.Dataset.from_generator(validation_data_generator, ({\"input_1\": tf.float32, \"input_2\": tf.float32}, tf.int64))\n",
    "validate_dataset = validate_dataset.batch(_BATCH_SIZE)\n",
    "validate_dataset = validate_dataset.repeat()\n",
    "validation_iterator = iter(validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with strategy.scope():\n",
    "with tf.device('/gpu:0'):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model('Models/dblp_db_model_'+str(model_version)+'.h5', compile=False)\n",
    "    except:\n",
    "        \n",
    "        inputs1 = tf.keras.Input(shape=(200,))\n",
    "        inputs2 = tf.keras.Input(shape=(968,))\n",
    "\n",
    "        x1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)(inputs2)\n",
    "        x2 = tf.keras.layers.Dense(128, activation=tf.nn.relu)(inputs1)\n",
    "        #d1 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "        x3 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x2)\n",
    "        #d2 = tf.keras.layers.Dropout(0.3)(x3)\n",
    "        concat_layer= tf.keras.layers.Concatenate()([x1,x3])\n",
    "        d2 = tf.keras.layers.Dropout(0.3)(concat_layer)\n",
    "        #d3 = tf.keras.layers.BatchNormalization()(concat_layer)\n",
    "        #x5 = tf.keras.layers.Dense(128, activation=tf.nn.sigmoid)(d3)\n",
    "        #d1 = tf.keras.layers.Dropout(0.2)(concat_layer)\n",
    "        outputs = tf.keras.layers.Dense(len(np.unique(y_train)), activation=tf.nn.softmax)(d2)\n",
    " \n",
    "        \n",
    "        model = tf.keras.Model(inputs={'input_1': inputs1, 'input_2':inputs2}, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train), y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "best_model_path = 'Models/dblp_db_model_'+str(model_version)+'.h5'\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=_PATIENCE,verbose=1)\n",
    "mc = ModelCheckpoint(filepath=best_model_path, monitor='val_accuracy', mode='max', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "plot_model(model,to_file='demo.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_iterator,steps_per_epoch=_STEPS_PER_EPOCH,\n",
    "                epochs=_EPOCHS,validation_data=validation_iterator,\n",
    "                validation_steps=_VALIDATION_STEPS,callbacks=[es, mc],class_weight=class_weights,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump accuracy and loss metrics\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss_file_path = \"Models/loss2_\"+str(model_version)+\".pickle\"\n",
    "with open(loss_file_path,'wb') as file:\n",
    "    pickle.dump(loss,file)\n",
    "\n",
    "val_loss_file_path = \"Models/val_loss2_\"+str(model_version)+\".pickle\"\n",
    "with open(val_loss_file_path,'wb') as file:\n",
    "    pickle.dump(val_loss,file)\n",
    "    \n",
    "acc_file_path = \"Models/acc2_\"+str(model_version)+\".pickle\"\n",
    "with open(acc_file_path,'wb') as file:\n",
    "    pickle.dump(acc,file)\n",
    "    \n",
    "val_acc_file_path = \"Models/val_acc2_\"+str(model_version)+\".pickle\"\n",
    "with open(val_acc_file_path,'wb') as file:\n",
    "    pickle.dump(val_acc,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
